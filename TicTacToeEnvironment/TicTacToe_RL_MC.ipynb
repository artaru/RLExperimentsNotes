{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90df2b7c",
   "metadata": {},
   "source": [
    "# **TicTacToe Reinforcement Learning and Monte Carlo Methods**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01092be6",
   "metadata": {},
   "source": [
    "## TicTacToe Environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab3e20",
   "metadata": {},
   "source": [
    "Will be using a custom TicTacToe environment based on the OpenAi gym framework.\n",
    "\n",
    "**Reward** is a tuple representing the reward each agent receives after the action is performed. The first value is the reward X receives and the second value is the reward O receives. We will use this reward to learn what states are good and bad.\n",
    "\n",
    "*   If X wins the reward is (10, -10)\n",
    "*   If O wins the reward is (-10, 10)\n",
    "*   If the game is a tie or not done yet the reward is (0, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265de9b4",
   "metadata": {},
   "source": [
    "## Setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563357e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.20.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym==0.20.0) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym==0.20.0) (2.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym==0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd7c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  gym-tictactoe.zip\r\n",
      "  inflating: gym-tictactoe/setup.py  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/PKG-INFO  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/SOURCES.txt  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/requires.txt  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/top_level.txt  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/dependency_links.txt  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/__init__.py  \r\n",
      "  inflating: gym-tictactoe/.ipynb_checkpoints/setup-checkpoint.py  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/__pycache__/__init__.cpython-39.pyc  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/.ipynb_checkpoints/__init__-checkpoint.py  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__init__.py  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/tictactoe_env.py  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__pycache__/__init__.cpython-39.pyc  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__pycache__/tictactoe_env.cpython-39.pyc  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/.ipynb_checkpoints/__init__-checkpoint.py  \r\n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/.ipynb_checkpoints/tictactoe_env-checkpoint.py  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o gym-tictactoe.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52cda9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/artem/Downloads/gym-tictactoe\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gym in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym-tictactoe==0.0.1) (0.20.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym->gym-tictactoe==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym->gym-tictactoe==0.0.1) (1.24.1)\n",
      "Installing collected packages: gym-tictactoe\n",
      "  Running setup.py develop for gym-tictactoe\n",
      "Successfully installed gym-tictactoe-0.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e gym-tictactoe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3ba79",
   "metadata": {},
   "source": [
    "## Importing Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7004fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym is a library that will allow us to initialize and work with the TicTacToe environment\n",
    "import gym\n",
    "\n",
    "# Random allows us to make random choices when interacting with the environment\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Custom tictactoe environment\n",
    "import gym_tictactoe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f0d6e",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749908ae",
   "metadata": {},
   "source": [
    "Initialize the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9940e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TicTacToe-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28ce9f",
   "metadata": {},
   "source": [
    "Visualize the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1fc119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33ba1a",
   "metadata": {},
   "source": [
    "Check the **action space** of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b37435d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a2971",
   "metadata": {},
   "source": [
    "Make a step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c50fb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('X--------', (0, 0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state, reward, done, info = env.step(0, \"X\")\n",
    "new_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054abd92",
   "metadata": {},
   "source": [
    "Define a new python function that will follow the epsilon probability and return an action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d99408f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(action,epsilon=0.1,avb_actions=[]):\n",
    "    ''' \n",
    "    This function takes the best estimated action, eplsilon, and action space \n",
    "    and returns some action. \n",
    "    '''\n",
    "    # generate a random number from 0 to 1.\n",
    "    number = np.random.rand(1)\n",
    "    \n",
    "    # if number is smaller than 1-epsilon then return best estimated action\n",
    "    if number<1-epsilon:\n",
    "        return action\n",
    "    # if number is bigger or equals to 1-epsilon then return some random action from the action space\n",
    "    else:\n",
    "        action=random.choice(avb_actions)  \n",
    "        return action "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb38c5f4",
   "metadata": {},
   "source": [
    "## Monte Carlo Algorithm - Train X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a49c50",
   "metadata": {},
   "source": [
    "Implement the MC algorithm, a few things to note: \n",
    "* We will be training $Agent_X$, so the one who plays as \"X\" on the board.\n",
    "* We will be training our agent against a random oponent (so oponent, $Agent_O$, will make random actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e03f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_X(environment,N_episodes=10000,epsilon=0.1, discount_factor=1):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy usistate=environment.reset() ng the epsilon greedy method. The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects. The class also Specifies if first-visit and every-visit method. The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy \n",
    "    Q funciton\n",
    "    \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    epsilon: the exploration factor\n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    \"\"\"\n",
    "    #dictionary of estimated action function for FrozenLake\n",
    "    Q={}\n",
    "    \n",
    "    # number of visits to the action function \n",
    "    NumberVisits= {}\n",
    "   \n",
    "    #dictionary  for policy \n",
    "    policy={}\n",
    "    \n",
    "    number_actions = 9\n",
    "    \n",
    "    for i in range(N_episodes):\n",
    "        \n",
    "        #list that stores each state and reward for each episode     \n",
    "        episode=[]\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        environment.reset() \n",
    "        state = environment.hash()\n",
    "        \n",
    "        #flag for end of episodes  \n",
    "        done=False\n",
    "        \n",
    "        #check if a policy for the state exists  \n",
    "        if state in policy.keys():\n",
    "            #obtain action from policy \n",
    "            action= policy[state]\n",
    "            action= random_action(action,epsilon,environment.available_actions())\n",
    "\n",
    "        else:\n",
    "            #if no policy for the state exists  select a random  action  \n",
    "            action=random.choice(environment.available_actions())\n",
    "            \n",
    "        #take action and find next state, reward\n",
    "        (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "        #append first state, reward and action\n",
    "        episode.append({'state':state, 'reward':reward[0],'action':action})\n",
    "        \n",
    "        #enumerate for each episode \n",
    "        while not(done):\n",
    "            #Agent O should make a random action.\n",
    "            (state, reward, done, info) = environment.step(random.choice(environment.available_actions()), \"O\")\n",
    "            \n",
    "            #Check if the episode is done (True) \n",
    "            if not done:\n",
    "                #Check if a policy for the state exists \n",
    "                if state in policy.keys():\n",
    "                    #obtain action from policy \n",
    "                    action=policy[state]\n",
    "                    action= random_action(action,epsilon,environment.available_actions())\n",
    "\n",
    "                else:\n",
    "                    #if no policy for the state exists  select a random  action  \n",
    "                    action=random.choice(environment.available_actions())\n",
    "                #take action and find next state, reward\n",
    "                (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "                \n",
    "            #add state reward and action to list \n",
    "            episode.append({'state':state, 'reward':reward[0],'action':action})\n",
    "        \n",
    "        #reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "\n",
    "        # determine the return\n",
    "        G=0\n",
    "        for t,step in enumerate(episode):\n",
    "                \n",
    "                G=discount_factor*G+step['reward']\n",
    "                \n",
    "                #increment counter for action \n",
    "                if (step['state'],step['action']) in NumberVisits.keys():\n",
    "                    #obtain action from policy \n",
    "                    NumberVisits[step['state'],step['action']]+=1\n",
    "\n",
    "                else:\n",
    "                    #if no policy for the state exists  select a random  action  \n",
    "                    NumberVisits[step['state'],step['action']]=1\n",
    "        \n",
    "                #if the action function value  does not exist, create an array  to store them \n",
    "                if not step['state'] in Q.keys():\n",
    "                    Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                #calculate mean of action function Q Value functions using the  recursive definition of mean \n",
    "                Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                \n",
    "                #setting the value of impossible statee-action pairs to -1000 (not the most elegant but the most efficient way in our simple case)\n",
    "                for i,x in enumerate(step['state']):\n",
    "                    if x != \"-\":\n",
    "                        Q[step['state']][i] = -1000\n",
    "                        \n",
    "                #update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "                policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        \n",
    "   \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea6660",
   "metadata": {},
   "source": [
    "Train it and see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3638cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_X, Q= monte_carlo_X(env,N_episodes=100000,epsilon=0.1, discount_factor=0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43082ac",
   "metadata": {},
   "source": [
    "Check the results of training and analyze how agent performs, for the following code the actions of X player will be dictated only by our policy, let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f799c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.8%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "test = 1000\n",
    "for i in range(test):\n",
    "    # variable to keep track of if the game is over\n",
    "    done = False\n",
    "    # Good practice to reset environment before you play a game to clear any old game\n",
    "    env.reset()\n",
    "    # Want to keep playing untill game is over\n",
    "    new_state = env.hash()\n",
    "    while not done:\n",
    "        # Make an action from policy for X\n",
    "        new_state, reward, done, info = env.step(policy_X[new_state], \"X\")\n",
    "\n",
    "        # If the game is done on X action we dont want O to make an action\n",
    "        if not done:\n",
    "            # Make a random action from the list of available actions for O\n",
    "            new_state, reward, done, info = env.step(random.choice(env.available_actions()), \"O\")\n",
    "        \n",
    "    total += reward[0]\n",
    "        \n",
    "# Print the reward after the game is done, reward for X is the first value and O is the second value\n",
    "print(f\"{total/test *10}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a424f5",
   "metadata": {},
   "source": [
    "Agent performs extraordinary well against random agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36212a8",
   "metadata": {},
   "source": [
    "## Monte Carlo Algorithm - Train O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94910dc1",
   "metadata": {},
   "source": [
    "Training notes: \n",
    "* We will be training $Agent-O$, so the one who plays as \"O\" on the board.\n",
    "* We will be training our agent against a random oponent (so oponent, $Agent-X$, will make random actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0679c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_O(environment,N_episodes=10000,epsilon=0.1, discount_factor=1):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy usistate=environment.reset() ng the epsilon greedy method. The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects. The class also Specifies if first-visit and every-visit method. The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy \n",
    "    Q funciton\n",
    "    \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    epsilon: the exploration factor\n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    \"\"\"\n",
    "    #dictionary of estimated action function for FrozenLake\n",
    "    Q={}\n",
    "    \n",
    "    # number of visits to the action function \n",
    "    NumberVisits= {}\n",
    "   \n",
    "    #dictionary  for policy \n",
    "    policy={}\n",
    "    \n",
    "    #number of possible actions\n",
    "    number_actions = 9\n",
    "    \n",
    "    for i in range(N_episodes):\n",
    "        \n",
    "        #list that stores each state and reward for each episode     \n",
    "        episode=[]\n",
    "        #reset the  environment for the next episode and find first state  \n",
    "        environment.reset() \n",
    "        state = environment.hash()\n",
    "        \n",
    "        #flag for end of episodes  \n",
    "        done=False\n",
    "        \n",
    "        #enumerate for each episode \n",
    "        while not(done):\n",
    "            #Agent X should make a random action\n",
    "            (state_1, reward, done, info) = environment.step(random.choice(environment.available_actions()), \"X\")\n",
    "            \n",
    "            #Check if the episode is done (True) \n",
    "            if not done: \n",
    "                if state_1 in policy.keys():\n",
    "                    #obtain action from policy \n",
    "                    action=policy[state_1]\n",
    "                    action= random_action(action,epsilon,environment.available_actions())\n",
    "\n",
    "                else:\n",
    "                    #if no policy for the state exists  select a random  action  \n",
    "                    action=random.choice(environment.available_actions())\n",
    "                #Agent O should make a given aciton.\n",
    "                (state, reward, done, info) = environment.step(action, \"O\")\n",
    "                \n",
    "                    \n",
    "            #add state reward and action to list \n",
    "            episode.append({'state':state_1, 'reward':reward[1],'action':action})\n",
    "        \n",
    "        #reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "\n",
    "        # determine the return\n",
    "        G=0\n",
    "        for t,step in enumerate(episode):\n",
    "                \n",
    "                G=discount_factor*G+step['reward']\n",
    "                \n",
    "                #increment counter for action \n",
    "                if (step['state'],step['action']) in NumberVisits.keys():\n",
    "                    #obtain action from policy \n",
    "                    NumberVisits[step['state'],step['action']]+=1\n",
    "\n",
    "                else:\n",
    "                    #if no policy for the state exists  select a random  action  \n",
    "                    NumberVisits[step['state'],step['action']]=1\n",
    "        \n",
    "                #if the action function value  does not exist, create an array  to store them \n",
    "                if not step['state'] in Q.keys():\n",
    "                    Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                #calculate mean of action function Q Value functions using the  recursive definition of mean \n",
    "                Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                 \n",
    "                #setting the value of impossible state-action pairs to -1000 (not the most elegant but the most efficient way in our simple case)\n",
    "                for i,x in enumerate(step['state']):\n",
    "                    if x != \"-\":\n",
    "                        Q[step['state']][i] = -1000\n",
    "                \n",
    "                #update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "                policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        \n",
    "   \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cbc2d",
   "metadata": {},
   "source": [
    "Train it as well and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fa46589",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_O, Q = monte_carlo_O(env,N_episodes=100000,epsilon=0.1, discount_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e296e39",
   "metadata": {},
   "source": [
    " Run it against the random X player: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9196cf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.4%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "test = 1000\n",
    "for i in range(test):\n",
    "    # variable to keep track of if the game is over\n",
    "    done = False\n",
    "    # Good practice to reset environment before you play a game to clear any old game\n",
    "    env.reset()\n",
    "    # Want to keep playing untill game is over\n",
    "    new_state = env.hash()\n",
    "    while not done:\n",
    "        # Make a random action from the list of available actions for X\n",
    "        new_state, reward, done, info = env.step(random.choice(env.available_actions()), \"X\")\n",
    "\n",
    "        # If the game is done on X action we dont want O to make an action\n",
    "        if not done:\n",
    "            # Make an action from the list of available actions for O\n",
    "            new_state, reward, done, info = env.step(policy_O[new_state], \"O\")\n",
    "        \n",
    "    total += reward[1]\n",
    "        \n",
    "    # Print the reward after the game is done, reward for X is the first value and O is the second value\n",
    "print(f\"{total/test *10}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba55770",
   "metadata": {},
   "source": [
    "Try putting one trained agent against another: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9624631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win: 0 Tie: 10000 O Win: 0\n",
      "X Win Rate: 0.0 Tie Rate: 100.0 O Win Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test(episodes):\n",
    "    # counters to keep track of results\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    # loops for a certain number of games (episodes)\n",
    "    for episode in range(episodes):\n",
    "        # stops while loop when game is done\n",
    "        done = False\n",
    "        # resets environment when game is done\n",
    "        env.reset()\n",
    "        new_state = env.hash()\n",
    "        # X agents turn\n",
    "        # performs an action\n",
    "        new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        while not done:\n",
    "            # O agents turn\n",
    "            # Get the best action\n",
    "            new_state, reward, done, _ = env.step(policy_O[new_state], \"O\")\n",
    "            \n",
    "            # if the game ends on O move, we don't want to make an X move\n",
    "            if (not done):\n",
    "                 # X agents turn\n",
    "                 # performs an action\n",
    "                new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        # record results when game is done\n",
    "        if (reward == (-10, 10)):\n",
    "            o_win+=1\n",
    "        elif (reward == (10, -10)):\n",
    "            x_win+=1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie+=1\n",
    "    return x_win, o_win, tie\n",
    "\n",
    "episodes = 10000\n",
    "\n",
    "x_win, o_win, tie = test(episodes)\n",
    "\n",
    "print(\"X Win:\", x_win, \"Tie:\", tie, \"O Win:\", o_win)\n",
    "print(\"X Win Rate:\", x_win/episodes*100, \"Tie Rate:\", tie/episodes*100, \"O Win Rate:\", o_win/episodes*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300d115",
   "metadata": {},
   "source": [
    "The function returns a perfect Tie Rate of $100\\%$ as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc84a8b",
   "metadata": {},
   "source": [
    "**Conclusion** : have created a successful algorithm, that solves a deterministic environment, against a similary trained agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f802b",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* [ Algorithms and pseudocode: Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction\"](https://www.amazon.ca/Reinforcement-Learning-Introduction-Second-Paperback/dp/B0B95WFGV6/ref=asc_df_B0B95WFGV6/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMDeveloperSkillsNetworkGPXX0HAUEN1421-2022-01-01&tag=googleshopc0c-20&linkCode=df0&hvadid=578919340205&hvpos=&hvnetw=g&hvrand=11011609051689383259&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9000828&hvtargid=pla-1728961664549&psc=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
