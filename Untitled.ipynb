{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9318d9b",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Notes\n",
    "**Reinforcement Learning** is based on rewarding desired actions/output and punishing for the undesired ones. Reinforcement learning models, just like people, are choosing which action to make based on the expected return of each action. You must give your model some input which include the information about current situation and possible actions, then you must reward it based on the output. Reinforcement learning models learn to perform a task through repeated trial and error interactions with a changing environment, without any human intervention.\n",
    " \n",
    "### Basic Terminology \n",
    "\n",
    "* **Agent**: is your reinforcement learning model, it's a decision maker and learner, \n",
    "\n",
    "* **Environment**: is a world around your agent, the agent learns and acts inside of it. The environment takes the action provided by the agent and returns the next\n",
    "**state** and the **reward**.\n",
    "\n",
    "* **State** : is a complete description of the state of the environment. \n",
    "\n",
    "* **Action**: is the way agent interacts with the environment, the moves that your agent can make. **Aciton Space** is the set of all possible actions. \n",
    "\n",
    "* **Reward**: is the feedback from the environment, it can be negative or positive and impacts the agent and serves as an indication to an agent of what you want it to achieve. Rewards are generally unknown and agents learn how to correctly estimate the reward.\n",
    "\n",
    "* **Policy**: is a rule used by an agent to decide what actions to take, given the specific state. It works as a map from state to some action or a set of probabilities for each action in the action space. \n",
    "\n",
    "\n",
    "* **Value Function**: is the function that returns the expected total reward your agent can get from following the specific policy. The agent uses this value function to make decisions and learns by updating the expected reward values of the parameters of this function. In this lab we will be using the  state-action value function, so our function $Q(s,a)$ will use a state-action pair and will return an estimated reward for taking action $a$ from state $s$. \n",
    "\n",
    "### Reinforcement Learning Process\n",
    "1. Agent plays a number of games\n",
    "2. In every game, the agent chooses an **Action** form the action space by using **Policy** and **Value Function**\n",
    "3. **Action** impacts the environment and the **Reward** and the new **State** is returned to the agent.\n",
    "4. The agent keeps track of what reward it received after choosing a certain action from a certain set. \n",
    "5. The after completing the game, the agent updates the estimated reward for each state and action by using the actual rewards values received while playing the game. \n",
    "6. The whole process repeats again.\n",
    "\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0O9IEN/reinforcement-learning-fig1-700.jpg\" width=\"30%\" alt=\"cheques image\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09694bca",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Policy\n",
    "Policy is just a function that defines which action our agent should take based on the current state. A simple deterministic policy $\\pi$ for the the state $(15,10,0)$ may look like: \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/Screenshot%202022-11-23%20at%2010.36.45%20AM.png\" width=\"50%\" alt=\"iris image\"  />\n",
    "\n",
    "**Epsilon** is some constant, ($0 \\leq \\epsilon \\leq 1$), and it will define some probability. **Greedy** implies that it will choose an action with the biggest estimated return. \n",
    "\n",
    "Assume that $Q(s,a)$ is our value function, it will return an **estimated** reward based on the given state and action and let $A$ be the action space. Then our policy can be simply defined: \n",
    "\n",
    "\n",
    "$$\n",
    "\\pi(s) =\n",
    "\\begin{cases}\n",
    "a = max_{a^* \\in A}Q(s,a^*)) & \\text{with probability 1-}  \\epsilon \\\\\\\\\n",
    "\\text{some }a \\in A & \\text{with probability } \\epsilon \\\\\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026680af",
   "metadata": {},
   "source": [
    "Why wouldn't we always use the action with the best estimated reward.\n",
    " \n",
    "* **Exploration** happens when the agent takes the random action to explore more opportunities, gather more information about possible actions and the environment.\n",
    "* **Exploitation** happens when the agent makes the best decision given current information, it uses the best estimated action to maximize the reward. \n",
    "\n",
    "**Epsilon** defines the trade-off between Exploration and Exploitation.  Best long-term strategy may involve short-term sacrifices and in most cases, agents must explore the environment and gather enough information to make the best overall decisions. It may save the agent from doing decisions that work instead of finding the best actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7468a",
   "metadata": {},
   "source": [
    "## Monte Carlo Method \n",
    "\n",
    "The basic principle of Monte Carlo method can be summarized in 4 steps: \n",
    "\n",
    "1. Define the Domain of Possible inputs \n",
    "2. Generate inputs randomly from a probability distribution over the domain\n",
    "3. Perform a deterministic computation on the inputs\n",
    "4. Average the results\n",
    "\n",
    "**Episode** is an agent-environment interactions from initial to final states which constists of steps in a in a discrete-time game. \n",
    "\n",
    "Monte Carlo reinforcement learning learns from **episodes of experience**, it functions by setting the value function equal to the empirical mean return.\n",
    "Let's assume that we have some initialized policy $\\pi$ that our agent follows. Then let's play a game once and gain the following episode: \n",
    "\n",
    "$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n",
    "\n",
    "Now let's look at an total expected reward of taking an Action $A_t$ in the state $S_t$, where t is some **time step**. \n",
    "\n",
    "* At **time step** $t=0$ (the first time step), the environment (including the agent) is in some state $S_t = S_0$ (the initial state), takes an action $A_t = A_0$ (the first action in the game) and receives a reward $R_t = R_0$ and the environment (including the agent) moves to a next state $S_{0+1} = S_{1}$\n",
    "\n",
    "Let's define a function $G$, which will just give us the expected total discounted reward at each time step: \n",
    "$$G(t) = R_t +\\gamma R_{t+1}+\\gamma^2 R_{t+2} + ...+ \\gamma^{k}R_{t+k}$$\n",
    "\n",
    "Discount factor $\\gamma \\in \\left[0,1\\right]$ is an important constant. We add the initial return $R_1$ as it is, without modifying the value, then to get the total reward we are adding $R_{t+1}$ but note that the value is multiplied by $0\\leq \\gamma \\leq 1$, so $R_{t+1}$ is only partially added to $R_1$, $R_{t+2}$ is multiplied by $\\gamma^2$, $R_{t+3}$ is multiplied by $\\gamma^3$ and so on. Gamma determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future. Note that if $\\gamma=0$ then total expected return will be defined just by initial reward, so agent will only learn and care about actions that produce an immediate reward. \n",
    "\n",
    "Now we can define our action-value function $Q_{\\pi}(S, A)$ for some sate $S$ and action $A$ as: \n",
    "$$Q_{\\pi}(S, A) = E_{\\pi}\\left[G(t)|S_t = S, A_t = A \\right ]$$\n",
    "\n",
    "So value function returns the expected value of a total discounted reward $G(t)$ for the time step $t$ at which $S_t = S$ and $A_t = A$. So, as an example, our state-value for the start state $S = 0$ should look something like:\n",
    "\n",
    "\n",
    "Now, after completing a series of episodes in the game, we will use the concept of **Incremental means** to adjust the expected values.\n",
    "\n",
    "**Incremental means**, is an average of values that's computed incrementally. Let $x_1, x_2,..., x_n$ be the set of values, Let $\\mu_1, \\mu_2, ... , \\mu_{n-1}$ be an sequence of means, where $\\mu_1 = x_1$, $\\mu_2 = \\dfrac{x_1+x_2}{2}$ and $\\mu_3 = \\dfrac{x_1+x_2+x_3}{3}$ and so on. Let's see how the mean is defined incrementally:\n",
    "\n",
    "\\begin{align*} \n",
    "\\mu_n &= \\frac{1}{n}\\sum_{i = 1}^{n}x_i\\\\  \n",
    "&= \\frac{1}{n} (x_n +  \\sum_{i = 1}^{n-1}x_i)\\\\\n",
    "&= \\frac{1}{n}(x_n +  (n-1)\\mu_{n-1})\\\\\n",
    "&= \\mu_{n-1} + \\frac{1}{n}(x_n - \\mu_{n-1})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbac45",
   "metadata": {},
   "source": [
    "Now we can put everything together to describe the Monte Calro Method Learning Process. Let's have an episode: \n",
    "$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n",
    "\n",
    "For each (state, action) pair we will keep track of the number of times this (state, action) was visited, let's define function $N(s_t,a_t)$, then every time we visit (state, action) we will update the visits counter and then adjust the running mean:\n",
    "\\begin{align*}\n",
    "N(S_t, A_t)&+=1\\\\\n",
    "Q(S_t, A_t)&+= \\frac{1}{N(S_t, A_t)}(G(t) - Q(S_t, A_t))\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5863271",
   "metadata": {},
   "source": [
    "## Improving Monte Carlo with First Starts:\n",
    "Every episode, we update our Q function, based on states and actions that were visited, some state-action pairs can be visited more than one per episode. **Every-Visit MC** is Monte Carlo algorithm that averages returns for every time state-action pair is visited in an episode, where **First-visit MC** averages returns only for first time state-action pair is visited in an episode. \n",
    "\n",
    "Let's check its implementation in the **First-visit MC** pseudo-code: \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0EP1EN/Screenshot%202022-11-23%20at%2012.33.51%20PM.png\" width=\"50%\" alt=\"iris image\"  />\n",
    "Where $T$ is the last step in the episode, $T-1$ is the second last one and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc699c3",
   "metadata": {},
   "source": [
    "## Improving Monte Carlo with Exploring-Starts:\n",
    "Note that in big and complicated environments not every (state,action) pair may be visited during the learning process. One possible solution to this problem may be adding **exploring-starts** method. In the beginning of each episode we are always starting in the initial state $S$, but with exploring-starts we will choose our starting state randomly. **Exploring-starts** is specifying that episodes start in a stateâ€“action pair, and that every pair has a nonzero probability of being selected as the start.\n",
    "\n",
    "Pseudo code: how exploring-starts can be implemented in to our Monte Carlo algorithm. \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX06PMEN/Screenshot%202022-11-09%20at%2012.24.33%20AM.png\" width=\"50%\" alt=\"iris image\"  />\n",
    "\n",
    "The only thing that changes in the code itself, is that the initial state of each episode is chosen randomly, the rest of Monte Carlo Algorithm remains unchanged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
